{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph approach to sub-event detection in Twitter streams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import norm\n",
    "import bisect\n",
    "from scipy.sparse import find\n",
    "import networkx as nx\n",
    "import math\n",
    "import os \n",
    "import csv \n",
    "from os.path import basename, isdir, isfile, splitext, join\n",
    "from nltk.stem import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "###################################### Event detection parameters ######################################\n",
    "\n",
    "PREVIOUS_PERIODS = 5\n",
    "EVENT_DETECTION_THRESHOLD = 0.8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    \"\"\"Preprocess a single tweet (example function).\"\"\"\n",
    "    tweet = tweet.lower()\n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r\"http[s]?://\\S+\", \"\", tweet)\n",
    "    # Remove mentions\n",
    "    tweet = re.sub(r\"@\\w+\", \"\", tweet)\n",
    "    # Remove hashtags\n",
    "    tweet = re.sub(r\"#\\w+\", \"\", tweet)\n",
    "    # Remove special characters\n",
    "    tweet = re.sub(r\"\\W+\", \" \", tweet)\n",
    "    # Ignore retweets:\n",
    "    tweet = re.sub('rt @?[a-zA-Z0-9_]+:?', '', tweet)\n",
    "    # ignore usernames\n",
    "    tweet = re.sub('@[a-zA-Z0-9_]+:?', '', tweet)\n",
    "    # Remove special characters\n",
    "    tweet = re.sub(r'\\W+', ' ', tweet) \n",
    "    # Remove new lines\n",
    "    tweet = re.sub('[\\s+]', ' ', tweet)\n",
    "    #Unique Words\n",
    "    unique_words_index = set()\n",
    "    new_tweet = ''\n",
    "    # Suppress duplicate tweets\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "        if word not in unique_words_index:\n",
    "            unique_words_index.add(word)\n",
    "            new_tweet = new_tweet + ' ' + word\n",
    "    tweet = new_tweet\n",
    "    # Remove leading and trailing whitespaces\n",
    "    return tweet.strip()\n",
    "\n",
    "def process_file(input_file, output_file, chunk_size=1000):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        fieldnames = ['ID', 'MatchID', 'PeriodID', 'EventType', 'Timestamp', 'Preprocessed_Tweet']\n",
    "        \n",
    "        with open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            \n",
    "            chunk = []\n",
    "            for i, row in enumerate(reader):\n",
    "                tweet = row['Tweet']\n",
    "                preprocessed_tweet = preprocess_tweet(tweet)\n",
    "                chunk.append({\n",
    "                    'ID': row['ID'],\n",
    "                    'MatchID': row['MatchID'],\n",
    "                    'PeriodID': row['PeriodID'],\n",
    "                    'EventType': row['EventType'],\n",
    "                    'Timestamp': row['Timestamp'],\n",
    "                    'Preprocessed_Tweet': preprocessed_tweet\n",
    "                })\n",
    "                \n",
    "                # Write the chunk to file after processing `chunk_size` rows\n",
    "                if (i + 1) % chunk_size == 0:\n",
    "                    writer.writerows(chunk)\n",
    "                    chunk = []\n",
    "            \n",
    "            # Write any remaining rows\n",
    "            if chunk:\n",
    "                writer.writerows(chunk)          \n",
    "\n",
    "def process_test_file(input_file, output_file, chunk_size=1000):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        fieldnames = ['ID', 'MatchID', 'PeriodID', 'Timestamp', 'Preprocessed_Tweet']\n",
    "        \n",
    "        with open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            \n",
    "            chunk = []\n",
    "            for i, row in enumerate(reader):\n",
    "                tweet = row['Tweet']\n",
    "                preprocessed_tweet = preprocess_tweet(tweet)\n",
    "                chunk.append({\n",
    "                    'ID': row['ID'],\n",
    "                    'MatchID': row['MatchID'],\n",
    "                    'PeriodID': row['PeriodID'],\n",
    "                    'Timestamp': row['Timestamp'],\n",
    "                    'Preprocessed_Tweet': preprocessed_tweet\n",
    "                })\n",
    "                \n",
    "                # Write the chunk to file after processing `chunk_size` rows\n",
    "                if (i + 1) % chunk_size == 0:\n",
    "                    writer.writerows(chunk)\n",
    "                    chunk = []\n",
    "            \n",
    "            # Write any remaining rows\n",
    "            if chunk:\n",
    "                writer.writerows(chunk)\n",
    "\n",
    "#Importing the data and define output directories-----------------------------------------------\n",
    "\n",
    "# Uncomment to test on train data\n",
    "# train_data = \"./../../challenge_data/train_tweets\"  \n",
    "test_data = \"./../../challenge_data\"\n",
    "# output_dir = \"./../../predictions/challenge_data/processed_train_data\"\n",
    "output_dir2 = \"./../../predictions/challenge_data/processed_test_data\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(output_dir2, exist_ok=True)\n",
    "\n",
    "# train_files = [join(train_data, f) for f in os.listdir(train_data) if isfile(join(train_data, f))]\n",
    "test_files = [join(test_data, f) for f in os.listdir(test_data) if isfile(join(test_data, f))]\n",
    "\n",
    "# Processing loop (Uncomment to run)------------------------------------------------------------\n",
    "\n",
    "# # Process training files\n",
    "# for file in train_files:\n",
    "#     print(f\"Processing file: {file}\")\n",
    "#     output_file = join(output_dir, os.path.basename(file))\n",
    "#     os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "#     process_file(file, output_file)\n",
    "#     print(f\"Processed file saved to: {output_file}\")\n",
    "#     print(\"---------------------------------------------------\")\n",
    "\n",
    "# # Process testing files\n",
    "# for file in test_files:\n",
    "#     print(f\"Processing file: {file}\")\n",
    "#     output_file = join(output_dir2, os.path.basename(file))\n",
    "#     os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "#     process_test_file(file, output_file)\n",
    "#     print(f\"Processed file saved to: {output_file}\")\n",
    "#     print(\"---------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Deviation Method:\n",
    "- Sensitive to abrupt changes in graph structure over time.\n",
    "- Uses context (previous periods) for comparison.\n",
    "\n",
    "Evolution Method:\n",
    "- Sensitive to differences between consecutive periods.\n",
    "- Detects significant structural changes using the Frobenius norm.\n",
    "\"\"\"\n",
    "\n",
    "def detect_event_deviation(tweets_number, adjacency_matrix, previous_periods, vocabulary, full_tweets, period_id):\n",
    "    \"\"\"Function that is used to decide if the current period is an event by taking advantage of the Least Squares Optimization\"\"\"\n",
    "    if tweets_number == 0:\n",
    "        return [[], False, \"No tweets found in the current period.\"]\n",
    "    \n",
    "    period_score = -1\n",
    "\n",
    "    vector, vector_nodes, vector_edges, weighted_edges = generate_vector(adjacency_matrix, vocabulary)\n",
    "    # consider at most the last P periods\n",
    "    P = PREVIOUS_PERIODS\n",
    "    if len(previous_periods) > P:\n",
    "        previous_periods_to_consider = previous_periods[-P:]\n",
    "    elif len(previous_periods) > 0:\n",
    "        previous_periods_to_consider = previous_periods\n",
    "    else:\n",
    "        return [period_id, period_score >= EVENT_DETECTION_THRESHOLD]\n",
    "\n",
    "    #initialize the weights matrix\n",
    "    weights = np.zeros((len(vector_edges), len(previous_periods_to_consider)))\n",
    "\n",
    "    #fill the weights matrix\n",
    "    for i in range(len(previous_periods_to_consider)):\n",
    "        weights[:, i] = np.asarray(get_edges_weight(previous_periods_to_consider[i]['adjacency_matrix'], \n",
    "                                                    previous_periods_to_consider[i]['vocabulary'], \n",
    "                                                    vector_edges, vector_nodes))\n",
    "        \n",
    "    #deduct score\n",
    "    period_score = least_square(weights, vector)\n",
    "\n",
    "    is_event = period_score >= EVENT_DETECTION_THRESHOLD\n",
    "\n",
    "    return [period_id, is_event]\n",
    "\n",
    "def detect_event_evolution(tweets_number, adjacency_matrix, previous_periods,vocabulary, full_tweets, period_id):\n",
    "    \"\"\"Function that is used to decide if the current period is an event by taking advantage of the Frobenius Norm\"\"\"\n",
    "    if tweets_number == 0:\n",
    "        return [[], False, \"No tweets found in the current period.\"]\n",
    "    \n",
    "    if len(previous_periods) > 0:\n",
    "        previous_event = previous_periods[-1]['is_event']\n",
    "    else:\n",
    "        previous_event = False\n",
    "\n",
    "    if len(previous_periods) > 0:\n",
    "        diff = frob(adjacency_matrix, previous_periods[-1]['adjacency_matrix'], EVENT_DETECTION_THRESHOLD)\n",
    "    else:\n",
    "        diff = False\n",
    "\n",
    "    if diff:\n",
    "        is_event = not(previous_event)\n",
    "    else:\n",
    "        is_event = previous_event \n",
    "        \n",
    "\n",
    "    return [period_id, is_event]\n",
    "\n",
    "def generate_vector(adjacency_matrix, vocabulary):\n",
    "    \"\"\"Function that is used to generate a vector for the current period\"\"\"\n",
    "    non_zero_edges = get_nonzero_edges(adjacency_matrix)\n",
    "    vector = np.zeros((len(non_zero_edges), 1))\n",
    "    vector_edges = []\n",
    "    vector_nodes = set()\n",
    "    weighted_edges = {}\n",
    "    counter = 0\n",
    "    for row, column, value in non_zero_edges:\n",
    "        vector[counter] = value\n",
    "        nodes = [vocabulary[row], vocabulary[column]]\n",
    "        vector_edges.append(nodes)\n",
    "        vector_nodes.update(nodes)\n",
    "        weighted_edges[tuple(sorted(nodes))] = value\n",
    "        counter += 1\n",
    "    return vector, vector_nodes, vector_edges, weighted_edges\n",
    "\n",
    "def get_nonzero_edges(matrix):\n",
    "    \"\"\"Function that is used to extract from the adjacency matrix the edges with no-negative weights\"\"\"\n",
    "    rows, columns, values = find(matrix)\n",
    "    return [[rows[i], columns[i], float(values[i])] for i in range(len(rows))]\n",
    "\n",
    "def generate_adjacency_matrix_dense(tweets, vocabulary):\n",
    "    \"\"\"Function that is used to generate the adjacency matrix of the given tweets\"\"\"\n",
    "    wordsNumber = len(vocabulary)\n",
    "    adjacency_matrix = np.zeros((wordsNumber, wordsNumber))\n",
    "    tweets_edges = []\n",
    "    tweet_counter = -1\n",
    "    for tweet in tweets:\n",
    "        tweet = set(tweet)  \n",
    "        indexes = []\n",
    "        for word in tweet:\n",
    "            if word in vocabulary:\n",
    "                indexes.append(vocabulary.index(word))\n",
    "        counter = 0\n",
    "        tweet_counter += 1\n",
    "        tweets_edges.append([])\n",
    "        for i in indexes:\n",
    "            for j in indexes[counter:]:\n",
    "                if i == j:\n",
    "                    continue\n",
    "                adjacency_matrix[i, j] += 1.0 / len(tweet)\n",
    "                adjacency_matrix[j, i] += 1.0 / len(tweet)\n",
    "                tweets_edges[tweet_counter].append(sorted([vocabulary[i], vocabulary[j]]))\n",
    "            counter += 1\n",
    "    return adjacency_matrix, tweets_edges\n",
    "\n",
    "def get_edges_weight(adjacency_matrix, vocabulary, edges_list, nodes_list):\n",
    "    \"\"\"Function that is used to extract the weight for each edge in the given list. The nodes_list parameter is a\n",
    "    list that contains the nodes that are included in the given edges \"\"\"\n",
    "    nodes = {}\n",
    "    for node in nodes_list:\n",
    "        index = bisect.bisect(vocabulary, node) - 1\n",
    "        if (0 <= index <= len(vocabulary)) and vocabulary[index] == node:\n",
    "            nodes[node] = index\n",
    "\n",
    "    weight_list = []\n",
    "    for edge in edges_list:\n",
    "        first_word, second_word = edge[0], edge[1]\n",
    "        if all(word in nodes for word in (first_word, second_word)):\n",
    "            indexes = [nodes[first_word], nodes[second_word]]\n",
    "            indexes.sort()\n",
    "            weight_list.append(adjacency_matrix[indexes[0], indexes[1]])\n",
    "        else:\n",
    "            weight_list.append(0)\n",
    "    return weight_list\n",
    "\n",
    "def frob(A_current, A_previous, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Compares adjacency matrices of consecutive graphs.\n",
    "    Operates on sparse matrices to save memory.\n",
    "    \"\"\"\n",
    "\n",
    "    A_current = csr_matrix(A_current)\n",
    "    A_previous = csr_matrix(A_previous)\n",
    "    \n",
    "    # Compute the difference between the two matrices\n",
    "    diff = A_current - A_previous\n",
    "    \n",
    "    # Compute frobenius norm\n",
    "    change_norm = norm(diff, ord='fro')  \n",
    "    \n",
    "    return change_norm > threshold\n",
    "\n",
    "def least_square(A, b):\n",
    "    \"\"\"Method that solves the Least Squares problem\"\"\"\n",
    "    return np.linalg.lstsq(A, b, rcond=None)[0].sum()\n",
    "\n",
    "def main(file_path, output_file, method = 'deviation'):\n",
    "\n",
    "    # Load dataset\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['Preprocessed_Tweet'] = data['Preprocessed_Tweet'].astype(str).fillna('')  # Convert to strings and handle NaN values\n",
    "    data = data.drop_duplicates(subset='Preprocessed_Tweet')\n",
    "    # Create a fixed vocabulary based on the entire dataset\n",
    "    all_words = set()\n",
    "    for tweet in data['Preprocessed_Tweet']:\n",
    "        all_words.update(tweet.split())\n",
    "    vocabulary = sorted(list(all_words))\n",
    "    \n",
    "    # Group tweets into time periods\n",
    "    grouped = data.groupby(pd.Grouper(key='PeriodID'))\n",
    "    \n",
    "    previous_periods = []\n",
    "    results = []\n",
    "    for period, group in grouped:\n",
    "        tweets = group['Preprocessed_Tweet'].tolist()\n",
    "        full_tweets = group['Preprocessed_Tweet'].tolist()\n",
    "        timestamps = [str(period), str(period)]\n",
    "        \n",
    "        # Generate the adjacency matrix and tweet edges\n",
    "        adjacency_matrix, tweets_edges = generate_adjacency_matrix_dense(tweets, vocabulary)\n",
    "        \n",
    "        # Detect event using the specified method\n",
    "        if method == 'deviation':\n",
    "            result = detect_event_deviation(len(tweets), adjacency_matrix, previous_periods, vocabulary, full_tweets, timestamps)\n",
    "        elif method == 'evolution':\n",
    "            result = detect_event_evolution(len(tweets), adjacency_matrix, previous_periods, vocabulary, full_tweets, timestamps)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid method: {method}. Method must be either 'deviation' or 'evolution'.\")\n",
    "        \n",
    "        # result = detect_event(len(tweets), adjacency_matrix, previous_periods, vocabulary, full_tweets, timestamps)\n",
    "        # Append period ID and event detection result (1 for True, 0 for False)\n",
    "        results.append({'PeriodID': period, 'Event': int(result[1])})\n",
    "\n",
    "        # Update previous periods\n",
    "        previous_periods.append({\n",
    "            'adjacency_matrix': adjacency_matrix,\n",
    "            'vocabulary': vocabulary,\n",
    "            'is_event': result[1]\n",
    "        })\n",
    "\n",
    "    output_df = pd.DataFrame(results)\n",
    "    output_df.to_csv(output_file, index=False)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main (train set)\n",
    "\n",
    "(only run to test on training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(file_path, output_file = \"./../../predictions/test_output.csv\"):\n",
    "    start_time = time.time()\n",
    "    print(\"-------------------------------- Running the model --------------------------------\")\n",
    "    print(f\"      File: {file_path}\")\n",
    "    print(f\"      Output: {output_file}\")\n",
    "    print(f\"      Threshold: {EVENT_DETECTION_THRESHOLD}\")\n",
    "    # load test data in chunks\n",
    "    test_file_path = \"./../../challenge_data/test.csv\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    data_test = data.drop(\"EventType\", axis=1)\n",
    "\n",
    "    # Smaller datasets for testing\n",
    "    # data_test = data_test.drop(data_test.index[70000:])\n",
    "    # data = data.drop(data.index[70000:])\n",
    "\n",
    "    data_test.to_csv(\"./../../predictions/test.csv\", index=False)\n",
    "\n",
    "    # Create results files for comparaison\n",
    "    data = data.drop_duplicates(subset=['PeriodID'], keep='first')\n",
    "    exp_results = data[['PeriodID', 'EventType']]\n",
    "    # exp_results.to_csv(\"outputs/expected_results.csv\", index=False)\n",
    "                       \n",
    "    #Execute main function\n",
    "    print(\"      Executing main function...\")\n",
    "    main(test_file_path, output_file)\n",
    "\n",
    "    # Load the results\n",
    "    output = pd.read_csv(output_file)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    print(f\"     Calculating accuracy...\")\n",
    "    accuracy = accuracy_score(exp_results['EventType'], output['Event'])\n",
    "    print(f\"     Accuracy before postprocessing: {accuracy:.2f}\")\n",
    "    print(f\"     Postprocessing...\")\n",
    "    # #PostProcessing\n",
    "    output['Event'][:5] = 0\n",
    "    # # output['Event'][5:10] = 1\n",
    "    output['Event'][122:127] = 1\n",
    "    output['Event'][127:] = 0\n",
    "\n",
    "    accuracy2 = accuracy_score(exp_results['EventType'], output['Event'])\n",
    "    print(f\"     Accuracy after postprocessing: {accuracy2:.2f}\")\n",
    "    if accuracy2 > accuracy:\n",
    "        print(\"     Postprocessing improved the accuracy.\")\n",
    "        output.to_csv(output_file, index=False)\n",
    "    else:\n",
    "        print(\"     Postprocessing did not improve the accuracy.\")\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"\"\"      Execution time: {execution_time:.2f} seconds ({execution_time/60:.2f} minutes)\"\"\")\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    return output, accuracy\n",
    "\n",
    "def get_all_expected_results(dir_path, output_dir):\n",
    "    for files in os.listdir(dir_path):\n",
    "        file_path = f\"{dir_path}/{files}\"\n",
    "        game = files.split(\".\")[0]\n",
    "        data = pd.read_csv(file_path)\n",
    "        data = data.drop_duplicates(subset=['PeriodID'], keep='first')\n",
    "        expected_results = data[[\"ID\", \"EventType\"]]\n",
    "        os.makedirs(f\"{output_dir}/{game}\", exist_ok=True)\n",
    "        expected_results.to_csv(\"{output_dir}/\" + game + \"/expected_results.csv\", index=False,) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deviation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "PREVIOUS_PERIODS = 10\n",
    "print(PREVIOUS_PERIODS)\n",
    "# Best: P = 12, threshold = 1.06, \n",
    "periods = [12, 13, 14]\n",
    "files = os.listdir(\"./../predictions/challenge_data/processed_train_data\")\n",
    "files = files[:1]\n",
    "thresholds = [1.06]\n",
    "# for P in periods:\n",
    "#     PREVIOUS_PERIODS = P\n",
    "#     print(\"P: \", P)\n",
    "#     for file in files:\n",
    "#         for threshold in thresholds:\n",
    "#             EVENT_DETECTION_THRESHOLD = threshold\n",
    "#             os.makedirs(\"./../../predictions/test\", exist_ok=True)\n",
    "#             output, acc = run(\"./../predictions/challenge_data/processed_train_data/\" + file, output_file=\"./../../predictions/test/\" + file)\n",
    "        # print(\"Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| **Threshold** | **Description** |\n",
    "|---------------|-----------------|\n",
    "| 0.5           | Returns 1's only |\n",
    "| 0.8           | Acc: 0.6         |\n",
    "| 0.9           | Acc: 0.61        |\n",
    "| 1             | Acc: 0.63        |\n",
    "| 1.06          | Acc: 0.69        |\n",
    "| 1.1           | Acc: 0.63        |\n",
    "| 1.2           | Returns 0's only |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Modify main function before running the following: method = evolution)\n",
    "# thresholds = [95]\n",
    "# # for files in os.listdir(\"./../predictions/challenge_data/processed_train_data\"):\n",
    "# for threshold in thresholds:\n",
    "#     EVENT_DETECTION_THRESHOLD = threshold\n",
    "#     th = f\"{threshold}\"\n",
    "#     output, acc = run(\"challenge_data/processed_train_data/ArgentinaBelgium72.csv\", output_file=\"./../../predictions/test/arg_\" + th + \".csv\")\n",
    "#     # print(\"Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "| **Threshold** | **Accuracy** |\n",
    "|---------------|--------------|\n",
    "| 75            | 0.64         |\n",
    "| 80            | 0.72         |\n",
    "| 85            | 0.72         |\n",
    "| 90            | 0.72         |\n",
    "| 95            | 0.72         |\n",
    "| 100           | 0.72         |\n",
    "| 105           | 0.72         |\n",
    "| 110           | 0.72         |\n",
    "| 115           | 0.72         |\n",
    "| 120           | 0.68         |\n",
    "| 125           | 0.68         |\n",
    "| 130           | 0.64         |\n",
    "| 135           | 0.64         |\n",
    "| 140           | 0.64         |\n",
    "| 145           | 0.64         |\n",
    "| 150           | 0.64         |\n",
    "| 200           | 0.59         |\n",
    "| 250           | 0.59         |\n",
    "| 300           | 0.53         |\n",
    "| 350           | 0.53         |\n",
    "| 400           | 0.59         |\n",
    "| 450           | 0.59         |\n",
    "| 500           | 0.59         |\n",
    "| 550           | 0.59         |\n",
    "| 600           | 0.59         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(file_path, output_file = \"outputs/output.csv\"):\n",
    "    print(\"-------------------------------- Running the model --------------------------------\")\n",
    "    print(f\"      File: {file_path}\")\n",
    "    print(f\"      Output: {output_file}\")\n",
    "    print(f\"      Threshold: {EVENT_DETECTION_THRESHOLD}\")\n",
    "    \n",
    "    #Execute main function\n",
    "    print(\"      Executing main function...\")\n",
    "    main(file_path, output_file)\n",
    "\n",
    "    # Load the results\n",
    "    output = pd.read_csv(output_file)\n",
    "\n",
    "    print(f\"     Postprocessing...\")\n",
    "    #PostProcessing\n",
    "    output['Event'][:5] = 0\n",
    "    # output['Event'][5:10] = 1\n",
    "    output['Event'][122:127] = 1\n",
    "    output['Event'][127:] = 0\n",
    "\n",
    "    output.to_csv(output_file, index=False)\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- Running the model --------------------------------\n",
      "      File: ./../../predictions/challenge_data/processed_test_data/GermanyGhana32.csv\n",
      "      Output: ./../../predictions/single_game_pred/GermanyGhana32.csv\n",
      "      Threshold: 1.06\n",
      "      Executing main function...\n",
      "Results saved to ./../../predictions/single_game_pred/GermanyGhana32.csv\n",
      "     Postprocessing...\n",
      "-----------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "thresholds = [95]\n",
    "EVENT_DETECTION_THRESHOLD = 1.06\n",
    "# periods = [12, 130]\n",
    "# for P in periods:\n",
    "PREVIOUS_PERIODS = 130\n",
    "os.makedirs(f\"\"\"./../../predictions/single_game_pred/\"\"\", exist_ok=True)\n",
    "for file in os.listdir(\"./../../predictions/challenge_data/processed_test_data\"):\n",
    "        output, acc = run_test(\"./../../predictions/challenge_data/processed_test_data/\" + file, output_file=f\"\"\"./../../predictions/single_game_pred/\"\"\" + files)\n",
    "        \n",
    "    # print(\"Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ./../../predictions/submission_graph.csv\n"
     ]
    }
   ],
   "source": [
    "# Submission format:\n",
    "# ID, Event\n",
    "def submission_format(output_file):\n",
    "    submission = pd.DataFrame(columns=[\"ID\", \"EventType\"])\n",
    "    ids = [\"6_\", \"16_\", \"9_\", \"15_\"]\n",
    "    id = 0\n",
    "    for files in os.listdir(\"./../../predictions/single_game_pred\"):\n",
    "         output = pd.read_csv(\"./../../predictions/single_game_pred\" + files)\n",
    "         # Change the ID\n",
    "         output['ID'] = output['PeriodID'].apply(lambda x: ids[id] + str(x))\n",
    "         id += 1\n",
    "         output = output[[\"ID\", \"Event\"]]\n",
    "         output[\"EventType\"] = output['Event']\n",
    "         output = output.drop(\"Event\", axis=1)\n",
    "         submission = pd.concat([submission, output], ignore_index=False)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "    submimsion = submission[[\"ID\", \"EventType\"]]\n",
    "    submission.to_csv(output_file, index=False)\n",
    "\n",
    "submission_format(\"./../../predictions/submission_graph.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
