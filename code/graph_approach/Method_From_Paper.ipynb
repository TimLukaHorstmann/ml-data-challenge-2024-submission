{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "from cvxpy import Variable, Minimize, Problem, norm\n",
    "from scipy.sparse import find\n",
    "from io import open\n",
    "import re\n",
    "from nltk.stem import *\n",
    "from collections import Counter\n",
    "import math\n",
    "import time\n",
    "from collections import deque\n",
    "from argparse import ArgumentParser\n",
    "from csv import reader\n",
    "from os.path import basename, isdir, isfile, splitext, join\n",
    "from sys import stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### Timestamp parameters ######################################\n",
    "EVENT_TIMESTAMPS = {}\n",
    "\n",
    "###################################### Preprocessing parameters ######################################\n",
    "IGNORE_URLS = True\n",
    "IGNORE_RETWEETS = True\n",
    "IGNORE_DUPLICATES = True\n",
    "IGNORE_USERNAME = True\n",
    "STEMMING = True\n",
    "TIME_WINDOW = -1\n",
    "\n",
    "###################################### Event detection parameters ######################################\n",
    "PREVIOUS_PERIODS = -1\n",
    "\n",
    "EVENT_DETECTION_THRESHOLD = {}\n",
    "\n",
    "BURST_DETECTION_THRESHOLD = {}\n",
    "###################################### Output parameters ######################################\n",
    "TWEETS_SUMMARY = -1\n",
    "SUMMARY_SIMILARITY = -1\n",
    "SUMMARY_PERIODS = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Util:\n",
    "    \"\"\"Class that is used to gather utility methods\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def beauty_print(to_print):\n",
    "        \"\"\"Method that is used to \"beautify\" a input list\"\"\"\n",
    "        if not to_print:\n",
    "            return []\n",
    "        max_len = len(max(to_print, key=len))\n",
    "        result = \"*\" * (max_len + 4) + \"\\n\"\n",
    "        for i in range(len(to_print)):\n",
    "            result += \"* \" + to_print[i].center(max_len) + \" *\" + \"\\n\"\n",
    "        result += \"*\" * (max_len + 4) + \"\\n\"\n",
    "        return str(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReadData.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reader:\n",
    "    \"\"\"Class that is used to read event files in a specified format\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.file_pointer = None\n",
    "        self.load_files = True\n",
    "        self.loaded_file_name = \"\"\n",
    "        self.loaded_clear_name = \"\" # <---\n",
    "        self.starting_timestamp = 0\n",
    "        self.file_periods, self.starting_tweets = [], []\n",
    "\n",
    "    def get_tweets(self, file_name):\n",
    "        \"\"\" The method that should be used to retrieve the tweets of the next time period\"\"\"\n",
    "        if self.loaded_file_name != file_name:\n",
    "            self.loaded_file_name = file_name\n",
    "            self.loaded_clear_name = file_name.split(\"/\")[-1]\n",
    "            self.load_files = os.stat(self.loaded_file_name).st_size < 524288000\n",
    "            if self.load_files:  # Load files with size less than 500MB)\n",
    "                self.load_file()\n",
    "            else:\n",
    "                self.starting_timestamp = Settings.EVENT_TIMESTAMPS.get(self.loaded_clear_name, [0, 0])[0]\n",
    "                self.starting_tweets = []\n",
    "                if self.file_pointer is not None:\n",
    "                    self.file_pointer.close()\n",
    "                self.file_pointer = open(file_name, 'rb')\n",
    "\n",
    "        return self.next_period() if self.load_files else self.read_next()\n",
    "\n",
    "    def next_period(self):\n",
    "        \"\"\"A method that is used to retrieve the next available period when a file is loaded in memory\"\"\"\n",
    "        return self.file_periods.pop(0) if len(self.file_periods) != 0 else None\n",
    "\n",
    "    def load_file(self):\n",
    "        \"\"\"Reads the specified file and returns a list with tweets for the next period or an empty list.\"\"\"\n",
    "        start_timestamp = Settings.EVENT_TIMESTAMPS.get(self.loaded_clear_name, [0, 0])[0]\n",
    "        end_timestamp = Settings.EVENT_TIMESTAMPS.get(self.loaded_clear_name, [0, 0])[1]\n",
    "\n",
    "        if end_timestamp != 0 and end_timestamp < start_timestamp:\n",
    "            return None\n",
    "        window_end_timestamp = start_timestamp + int(Settings.TIME_WINDOW)\n",
    "        with open(self.loaded_file_name, 'rb') as f:\n",
    "            reader = csv.reader(f)\n",
    "            temp_tweets = []\n",
    "            tweets = []\n",
    "            for row in reader:\n",
    "                if start_timestamp == 0:\n",
    "                    start_timestamp = int(row[0])\n",
    "                    window_end_timestamp = start_timestamp + int(Settings.TIME_WINDOW)\n",
    "                if int(row[0]) < start_timestamp:\n",
    "                    continue\n",
    "                if int(row[0]) > window_end_timestamp:\n",
    "                    tweets.append(temp_tweets)\n",
    "                    temp_tweets = []\n",
    "                    start_timestamp = window_end_timestamp\n",
    "                    window_end_timestamp += Settings.TIME_WINDOW\n",
    "                    while int(row[0]) > window_end_timestamp:\n",
    "                        tweets.append(temp_tweets)\n",
    "                        temp_tweets = []\n",
    "                        start_timestamp = window_end_timestamp\n",
    "                        window_end_timestamp += Settings.TIME_WINDOW\n",
    "                if int(row[0]) > end_timestamp > 0:\n",
    "                    break\n",
    "                temp_tweets.append(row)\n",
    "        if temp_tweets:\n",
    "            tweets.append(temp_tweets)\n",
    "        self.file_periods = tweets\n",
    "\n",
    "    def read_next(self):\n",
    "        \"\"\"Reads the specified file and returns a list with tweets for the next period or an empty list.\"\"\"\n",
    "        tweets = self.starting_tweets\n",
    "        self.starting_tweets = []\n",
    "        event_end_timestamp = Settings.EVENT_TIMESTAMPS.get(self.loaded_clear_name, [0, 0])[1]\n",
    "        window_end_timestamp = self.starting_timestamp + int(Settings.TIME_WINDOW)\n",
    "\n",
    "        if len(tweets) != 0 and int(tweets[0][1]) > window_end_timestamp:\n",
    "            tweets = []\n",
    "        limit = int(tweets[0][1]) if len(tweets) == 1 else 0\n",
    "        if 0 < event_end_timestamp <= max(self.starting_timestamp, limit) or self.file_pointer.closed:\n",
    "            self.file_pointer.close()\n",
    "            return None\n",
    "\n",
    "        reader = csv.reader(self.file_pointer)\n",
    "        eof = True\n",
    "        for row in reader:\n",
    "            if self.starting_timestamp == 0:\n",
    "                self.starting_timestamp = int(row[0])\n",
    "                window_end_timestamp = self.starting_timestamp + int(Settings.TIME_WINDOW)\n",
    "\n",
    "            if int(row[0]) < self.starting_timestamp:\n",
    "                continue\n",
    "\n",
    "            if int(row[0]) > window_end_timestamp or (int(row[0]) > event_end_timestamp > 0):\n",
    "                self.starting_tweets = [row]\n",
    "                self.starting_timestamp = window_end_timestamp\n",
    "                eof = False\n",
    "                break\n",
    "            tweets.append(row)\n",
    "        if eof:\n",
    "            self.file_pointer.close()\n",
    "        return tweets\n",
    "\n",
    "    @staticmethod\n",
    "    def count_tweets(file_name):\n",
    "        from Preprocessor import Preprocessor\n",
    "        from csv import reader\n",
    "        from time import time\n",
    "        with open(file_name, 'rb') as f:\n",
    "            for row in reader(f):\n",
    "                Settings.EVENT_TIMESTAMPS[file_name] = [int(row[0]), int(round(time() * 1000))]\n",
    "                break\n",
    "        Settings.TIME_WINDOW = 1 * 60000\n",
    "        reader = Reader()\n",
    "        c = 0\n",
    "        while True:\n",
    "            tweets = reader.get_tweets(file_name)\n",
    "            if tweets is None:\n",
    "                break\n",
    "            elif len(tweets) == 0:\n",
    "                continue\n",
    "            tweets, full_tweets, tweets_timestamps, vocabulary, timestamps = Preprocessor.preprocess(tweets)\n",
    "            c += len(tweets)\n",
    "        return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Preprocessor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \"\"\"Class that is used to preprocess the input tweets\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    stopwords = []\n",
    "    stemmer = None\n",
    "\n",
    "    @staticmethod\n",
    "    def contains_url(tweet):\n",
    "        \"\"\"Method that is used to check if a tweet contains a url\"\"\"\n",
    "        return len(re.findall(r\"http[s]?\\S+\", tweet)) != 0\n",
    "\n",
    "    @staticmethod\n",
    "    def is_retweet(tweet):\n",
    "        \"\"\"Method that is used to check if a tweet is retweet\"\"\"\n",
    "        return len(re.findall('rt @?[a-zA-Z0-9_]+:? .*', tweet)) != 0\n",
    "\n",
    "    @staticmethod\n",
    "    def contains_username(tweet):\n",
    "        \"\"\"Method that is used to check if a tweet contains the character \"@\", therefore refers to a user\"\"\"\n",
    "        return '@' in tweet\n",
    "\n",
    "    @classmethod\n",
    "    def preprocess(cls, tweets):\n",
    "        \"\"\"Method that is used to preprocess the incoming tweet. Returns a list with the preprocessed tweets, the full\n",
    "        text of the tweets that are returned, the corresponding timestamps and the vocabulary of all the tweets\"\"\"\n",
    "        if not cls.stopwords:\n",
    "            cls.stemmer = PorterStemmer()\n",
    "            cls.stopwords = [re.sub('[\\s+]', ' ', word.decode(\"utf-8-sig\").encode(\"utf-8\")).strip() for word in\n",
    "                             open(\"stop_words.txt\", 'rb')]\n",
    "        full_tweets, timestamps, result = [], [], []\n",
    "        unique_words_index = set()\n",
    "        for tweet in tweets:\n",
    "            timestamp = tweet[0]\n",
    "            tweet = tweet[1].lower()  # Keep the text part\n",
    "            if (Settings.IGNORE_URLS and cls.contains_url(tweet)) or \\\n",
    "                    (Settings.IGNORE_RETWEETS and cls.is_retweet(tweet)) or \\\n",
    "                    (Settings.IGNORE_USERNAME and cls.contains_username(tweet)):\n",
    "                continue\n",
    "            if not Settings.IGNORE_URLS and cls.contains_url(tweet):\n",
    "                tweet = re.sub(r\"http[s]?\\S+\", \"\", tweet)\n",
    "            if not Settings.IGNORE_RETWEETS and cls.is_retweet(tweet):\n",
    "                tweet = re.sub('rt @?[a-zA-Z0-9_]+:?', '', tweet)\n",
    "            if not Settings.IGNORE_USERNAME and cls.contains_username(tweet):\n",
    "                tweet = re.sub('@[a-zA-Z0-9_]+:?', '', tweet)\n",
    "\n",
    "            tweet = re.sub(r'\\W+', ' ', tweet)  # Remove special characters\n",
    "            tweet = re.sub('[\\s+]', ' ', tweet).strip()  # Remove spaces and new lines\n",
    "\n",
    "            full_tweet = tweet\n",
    "\n",
    "            tweet = [word for word in tweet.split(\" \") if cls.stopwords[bisect.bisect(cls.stopwords, word) - 1] != word]\n",
    "            if Settings.STEMMING:\n",
    "                tweet = [cls.stemmer.stem(word) for word in tweet]\n",
    "\n",
    "            if not Settings.IGNORE_DUPLICATES or tweet not in result:\n",
    "                unique_words_index.update(tweet)\n",
    "                result.append(tweet)\n",
    "                full_tweets.append(full_tweet)\n",
    "                timestamps.append(timestamp)\n",
    "        return result, full_tweets, timestamps, sorted(unique_words_index), [tweets[0][1], tweets[-1][1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Period.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Period:\n",
    "    \"\"\"Class that is used to represent a time period and to detect sub-events inside it\"\"\"\n",
    "\n",
    "    file_name = \"\"\n",
    "\n",
    "    def __init__(self, tweets, file_name, printing=True, write=True):\n",
    "        self.tweets_edges = []\n",
    "        self.adjacency_matrix = None\n",
    "        tweets, self.full_tweets, self.tweets_timestamps, self.vocabulary, self.timestamps = Preprocessor.preprocess(\n",
    "            tweets)\n",
    "\n",
    "        self.tweets_number = len(tweets)\n",
    "        self.generate_adjacency_matrix_dense(tweets)\n",
    "        self.toPrint = [\"Period starts: \" + self.timestamps[0] + \" Period ends: \" + self.timestamps[1],\n",
    "                        \"Number of tweets: \" + str(self.tweets_number),\n",
    "                        \"Vocabulary size: \" + str(len(self.vocabulary))]\n",
    "        self.write = write\n",
    "        self.printing = printing\n",
    "        Period.file_name = file_name\n",
    "\n",
    "    def generate_adjacency_matrix_dense(self, tweets):\n",
    "        \"\"\"Method that is used to generate the adjacency matrix of the given tweets\"\"\"\n",
    "        wordsNumber = len(self.vocabulary)\n",
    "        adjacency_matrix = np.zeros((wordsNumber, wordsNumber))\n",
    "        tweet_counter = -1\n",
    "        for tweet in tweets:\n",
    "            tweet = set(tweet)  # Duplicates\n",
    "            indexes = [bisect.bisect_left(self.vocabulary, word) for word in tweet]\n",
    "            counter = 0\n",
    "            tweet_counter += 1\n",
    "            self.tweets_edges.append([])\n",
    "            for i in indexes:\n",
    "                for j in indexes[counter:]:\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    adjacency_matrix[i, j] += 1.0 / len(tweet)\n",
    "                    adjacency_matrix[j, i] += 1.0 / len(tweet)\n",
    "                    self.tweets_edges[tweet_counter].append(sorted([self.vocabulary[i], self.vocabulary[j]]))\n",
    "                counter += 1\n",
    "        self.adjacency_matrix = adjacency_matrix\n",
    "\n",
    "    def get_edges_weight(self, edges_list, nodes_list):\n",
    "        \"\"\"Method that is used to extract the weight for each edge in the given list. The nodes_list parameter is a\n",
    "        list that contains the nodes that are included in the given edges \"\"\"\n",
    "        nodes = {}\n",
    "        for node in nodes_list:\n",
    "            index = bisect.bisect(self.vocabulary, node) - 1\n",
    "            if (0 <= index <= len(self.vocabulary)) and self.vocabulary[index] == node:\n",
    "                nodes[node] = index\n",
    "\n",
    "        weight_list = []\n",
    "        for edge in edges_list:\n",
    "            first_word, second_word = edge[0], edge[1]\n",
    "            if all(word in nodes for word in (first_word, second_word)):\n",
    "                indexes = [nodes[first_word], nodes[second_word]]\n",
    "                indexes.sort()\n",
    "                weight_list.append(self.adjacency_matrix[indexes[0], indexes[1]])\n",
    "            else:\n",
    "                weight_list.append(0)\n",
    "        return weight_list\n",
    "\n",
    "    @staticmethod\n",
    "    def get_nonzero_edges(matrix):\n",
    "        \"\"\"Method that is used to extract from the adjacency matrix the edges with no-negative weights\"\"\"\n",
    "        rows, columns, values = find(matrix)\n",
    "        return [[rows[i], columns[i], float(values[i])] for i in range(len(rows))]\n",
    "\n",
    "    def generate_vector(self):\n",
    "        \"\"\"Method that is used to generate a vector for the current period\"\"\"\n",
    "        non_zero_edges = self.get_nonzero_edges(self.adjacency_matrix)\n",
    "        vector = np.zeros((len(non_zero_edges), 1))\n",
    "        vector_edges = []\n",
    "        vector_nodes = set()\n",
    "        weighted_edges = {}\n",
    "        counter = 0\n",
    "        for row, column, value in non_zero_edges:\n",
    "            vector[counter] = value\n",
    "            nodes = [self.vocabulary[row], self.vocabulary[column]]\n",
    "            vector_edges.append(nodes)\n",
    "            vector_nodes.update(nodes)\n",
    "            weighted_edges[tuple(sorted(nodes))] = value\n",
    "            counter += 1\n",
    "        return vector, vector_nodes, vector_edges, weighted_edges\n",
    "\n",
    "    def detect_event(self, previous_periods):\n",
    "        \"\"\"Method that is used to decide if the current period is an event by taking advantage of the Least Squares\n",
    "        Optimization \"\"\"\n",
    "        if self.tweets_number == 0:\n",
    "            return [[], False, \"No tweets found in the current period.\"]\n",
    "        period_score = -1\n",
    "        vector, vector_nodes, vector_edges, weighted_edges = self.generate_vector()\n",
    "        repr_tweets = self.submodular_tweets(weighted_edges, [], [], Settings.TWEETS_SUMMARY)\n",
    "        if len(previous_periods) != 0:\n",
    "            weights = np.zeros((len(vector_edges), len(previous_periods)))\n",
    "            for i in range(len(previous_periods)):\n",
    "                weights[:, i] = np.asarray(previous_periods[i].get_edges_weight(vector_edges, vector_nodes))\n",
    "\n",
    "            period_score = Period.optimize(weights, vector)\n",
    "\n",
    "        is_event = period_score >= Settings.EVENT_DETECTION_THRESHOLD[Period.file_name.split(\"/\")[-1][:-4] + \".csv\"]\n",
    "        self.toPrint += [\" \"] + repr_tweets + [\" \"] + [\"Result: \" + str(is_event) + \" \" + str(period_score)]\n",
    "        if self.printing:\n",
    "            print Util.beauty_print(self.toPrint)\n",
    "\n",
    "        if self.write:\n",
    "            f = open(Period.file_name, 'a')\n",
    "            f.write(Util.beauty_print([\"Timestamp: \" + ' '.join(self.timestamps)] + repr_tweets +\n",
    "                                      [\"-------------------------------------------\"] +\n",
    "                                      [\"Result: \" + str(is_event)] +\n",
    "                                      [\"Score: \" + str(period_score)]))\n",
    "            f.close()\n",
    "        timestamps = self.timestamps\n",
    "        del self.full_tweets\n",
    "        del self.timestamps\n",
    "        del self.toPrint\n",
    "        del self.tweets_edges\n",
    "        del self.tweets_number\n",
    "        del self.tweets_timestamps\n",
    "        return [timestamps, is_event, \". \".join(repr_tweets) + \" || \" + str(period_score)]\n",
    "\n",
    "    @staticmethod\n",
    "    def optimize(A, b):\n",
    "        \"\"\"Method that solves the Least Squares problem\"\"\"\n",
    "        x = Variable(A.shape[1])\n",
    "        objective = Minimize(norm(A * x - b))  # Minimize(norm(A * x - b) + lamp * norm(x, p=1))\n",
    "        constraints = [0 <= x, sum(x) == 1]\n",
    "        minimum = Problem(objective, constraints).solve()\n",
    "        value = A.dot(x.value) - b\n",
    "        value[value > 0] = 0\n",
    "        minimum = np.linalg.norm(value)\n",
    "        return minimum\n",
    "\n",
    "    def submodular_tweets(self, weighted_edges, excluded_tweets, excluded_edges, tweets_number):\n",
    "        \"\"\"Method that is used to generate a summary for the current period\"\"\"\n",
    "        if tweets_number == 0:\n",
    "            return []\n",
    "        tweets_ranking = [0] * self.tweets_number\n",
    "        for tweet_counter in range(len(self.tweets_edges)):\n",
    "            if tweet_counter in excluded_tweets:\n",
    "                continue\n",
    "            tweet_edges = self.tweets_edges[tweet_counter]\n",
    "            tweet_value = 0\n",
    "            for edge in tweet_edges:\n",
    "                if tuple(edge) not in excluded_edges:\n",
    "                    tweet_value += weighted_edges[tuple(edge)]\n",
    "            tweets_ranking[tweet_counter] = tweet_value\n",
    "        sorted_indexes = sorted(range(len(tweets_ranking)), key=lambda i: tweets_ranking[i], reverse=True)\n",
    "        tweet_index = sorted_indexes[:Settings.TWEETS_SUMMARY][0]\n",
    "        excluded_tweets.append(tweet_index)\n",
    "        excluded_edges += [tuple(edge) for edge in self.tweets_edges[tweet_index]]\n",
    "        return [self.full_tweets[tweet_index]] + self.submodular_tweets(weighted_edges, excluded_tweets, excluded_edges,\n",
    "                                                                        tweets_number - 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SummaryWriter.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryWriter:\n",
    "    \"\"\"A class that is used to print the summary of a event\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def write_summary(cls, file_name, sentences, indexes):\n",
    "        \"\"\"A method that given a list of sentences and their indexes will write to the specified file the summary. To\n",
    "        accomplish that, cosine similarity is employed to remove sentences that look the same as previous sentences\"\"\"\n",
    "        sentences = [sentence.split(\"||\")[0] for sentence in sentences]\n",
    "        summary = [sentences[0]]\n",
    "        indexes_set = set(indexes)\n",
    "        for i in range(1, len(sentences)):\n",
    "            intersection = set(range(indexes[i] - Settings.SUMMARY_PERIODS, indexes[i])).intersection(indexes_set)\n",
    "            similarities = [cls.cosine_similarity(sentences[i], sentences[indexes.index(val)]) for val in intersection]\n",
    "            if (similarities and max(similarities) <= Settings.SUMMARY_SIMILARITY) or not similarities:\n",
    "                summary.append(sentences[i])\n",
    "        if file_name is not None:\n",
    "            f = open(file_name, 'w')\n",
    "            f.write(Util.beauty_print(summary))\n",
    "            f.close()\n",
    "        else:\n",
    "            return summary\n",
    "\n",
    "    @classmethod\n",
    "    def cosine_similarity(cls, sentence1, sentence2):\n",
    "        \"\"\"A method that is used to calculate the cosine similarity between two sentences\"\"\"\n",
    "        vec1 = Counter(re.compile(r'\\w+').findall(sentence1))\n",
    "        vec2 = Counter(re.compile(r'\\w+').findall(sentence2))\n",
    "        intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "        numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "        denominator1 = math.sqrt(sum([vec1[x] ** 2 for x in vec1.keys()]))\n",
    "        denominator2 = math.sqrt(sum([vec2[x] ** 2 for x in vec2.keys()]))\n",
    "\n",
    "        return float(numerator) / (denominator1 * denominator2) if (denominator1 * denominator2) != 0 else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BurstEventDetector.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BurstEventDetector:\n",
    "    \"\"\"Event detector that determines if a period is a sub-event based on the number of tweets\"\"\"\n",
    "\n",
    "    total_tweets = {}\n",
    "    file_name = \"\"\n",
    "\n",
    "    def __init__(self, tweets, file_name, printing=True, write=True):\n",
    "        self.tweets, self.full_tweets, self.tweets_timestamps, self.vocabulary, self.timestamps = Preprocessor.preprocess(tweets)\n",
    "        self.tweets_number = len(self.tweets)\n",
    "        self.file_name = file_name\n",
    "        self.write = write\n",
    "        self.printing = printing\n",
    "        BurstEventDetector.total_tweets[file_name] = BurstEventDetector.total_tweets.get(file_name, 0) + self.tweets_number\n",
    "        BurstEventDetector.file_name = file_name\n",
    "\n",
    "    def detect_event(self):\n",
    "        \"\"\"Method that is used to decide if the current period is a sub-event or not\"\"\"\n",
    "        toPrint = [\"Period starts: \" + self.timestamps[0] + \" Period ends: \" + self.timestamps[1],\n",
    "                   \"Number of tweets: \" + str(self.tweets_number),\n",
    "                   \"Vocabulary size: \" + str(len(self.vocabulary))]\n",
    "\n",
    "        is_event = self.contains_event()\n",
    "        period_score = self.tweets_number\n",
    "        toPrint += [\" No summary during baseline\"]\n",
    "        toPrint += [\" \"] + [\"Burst detector result: \" + str(self.contains_event())]\n",
    "        if self.printing:\n",
    "            print Util.beauty_print(toPrint)\n",
    "        if self.write:\n",
    "            f = open(BurstEventDetector.file_name, 'a')\n",
    "            f.write(Util.beauty_print([\"Timestamp: \" + ' '.join(self.timestamps)] + [\"No summary during baseline\"] +\n",
    "                                      [\"-------------------------------------------\"] +\n",
    "                                      [\"Result: \" + str(is_event)] +\n",
    "                                      [\"Score: \" + str(period_score)]))\n",
    "            f.close()\n",
    "        return [self.timestamps, is_event, \"No summary during baseline || \" + str(self.tweets_number)]\n",
    "\n",
    "    def contains_event(self):\n",
    "        \"\"\"Returns True if the number of tweets in this period is greater than the specified threshold\"\"\"\n",
    "        return self.tweets_number >= Settings.BURST_DETECTION_THRESHOLD[self.file_name.split(\"/\")[-1][:-3] + \"csv\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detection_process.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(args):\n",
    "    script_time = time.time()\n",
    "    reader = Reader()\n",
    "    file_name = splitext(basename(args.input))[0]\n",
    "    results_file = join(args.output, file_name + \".txt\")\n",
    "    summary_file = join(args.output, file_name + \"_summary.txt\")\n",
    "    open(results_file, 'w').close()\n",
    "    open(summary_file, 'w').close()\n",
    "    print \"Starting the processing of file:\", file_name\n",
    "    iterations = 0\n",
    "    periods_window = deque(maxlen=args.periods)\n",
    "    summary, summary_index = [], []\n",
    "    while True:\n",
    "        iterations += 1\n",
    "        start_time = time.time()\n",
    "        tweets = reader.get_tweets(args.input)\n",
    "        if tweets is None:\n",
    "            break\n",
    "        elif len(tweets) == 0:\n",
    "            if args.v:\n",
    "                print \"Time period\", iterations, \"was skipped because there was 0 tweets published.\"\n",
    "            continue\n",
    "        if args.v:\n",
    "            print \"#\" * 25, \"Time period\", iterations, \"#\" * 25\n",
    "\n",
    "        if args.burst:\n",
    "            period = BurstEventDetector(tweets, results_file, args.v)\n",
    "            event = period.detect_event()\n",
    "        else:\n",
    "            period = Period(tweets, results_file, args.v)\n",
    "            event = period.detect_event(periods_window)\n",
    "        if event[1]:\n",
    "            summary.append(event[2])\n",
    "            summary_index.append(iterations)\n",
    "        periods_window.append(period)\n",
    "        if args.v:\n",
    "            print \"Time period #\", iterations, \", Completion time:\", time.time() - start_time\n",
    "    SummaryWriter.write_summary(summary_file, summary, summary_index)\n",
    "\n",
    "    print \"Total time:\", time.time() - script_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"input\", help=\"path to the file containing the tweets that will be used as input\", type=str)\n",
    "    parser.add_argument(\"--output\", help=\"path to the directory where the results would be stored\", default=\"./output\",\n",
    "                        type=str)\n",
    "    parser.add_argument(\"--threshold\",\n",
    "                        help=\"the threshold that will be used to determine if a period contains a sub-event\",\n",
    "                        default=0, type=float)\n",
    "    parser.add_argument(\"--periods\", help=\"number of previous periods that are considered (default 10)\", default=10,\n",
    "                        type=int)\n",
    "    parser.add_argument(\"--duration\", help=\"duration of the period set in minutes (default 1 min)\", default=1,\n",
    "                        type=float)\n",
    "\n",
    "    parser.add_argument(\"--urls\", help=\"do not remove urls during preprocessing\", action=\"store_false\")\n",
    "    parser.add_argument(\"--retweets\", help=\"do not remove retweets during preprocessing\", action=\"store_false\")\n",
    "    parser.add_argument(\"--mentions\", help=\"do not remove users' mentions during preprocessing\", action=\"store_false\")\n",
    "    parser.add_argument(\"--stemming\", help=\"do not perform stemming during preprocessing\", action=\"store_false\")\n",
    "    parser.add_argument(\"--duplicates\", help=\"do not remove duplicate tweets during preprocessing\",\n",
    "                        action=\"store_false\")\n",
    "\n",
    "    parser.add_argument(\"--burst\", help=\"use the burst detector instead of the optimized method\", action=\"store_true\")\n",
    "    parser.add_argument(\"--v\", help=\"increase output verbosity\", action=\"store_true\")\n",
    "\n",
    "    parser.add_argument(\"--summary\", help=\"number of tweets that the summary will contains (default 2)\", default=2,\n",
    "                        type=int)\n",
    "    parser.add_argument(\"--sp\", help=\"number of previous periods where the reappearance of an sub-event is considered \"\n",
    "                                     \"duplicate (default 3)\", default=3, type=int)\n",
    "    parser.add_argument(\"--c\", help=\"cosine similarity threshold to compare the summaries (default 0.6)\", default=0.6,\n",
    "                        type=float)\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def argument_check():\n",
    "    args = parse_arguments()\n",
    "    if not isfile(args.input):\n",
    "        stderr.write('The specified input file ' + str(args.input) + ' was not found.')\n",
    "        raise SystemExit(1)\n",
    "    elif not isdir(args.output):\n",
    "        stderr.write('The specified parameter should be a directory name, not a file.')\n",
    "        raise SystemExit(1)\n",
    "    elif args.duration <= 0:\n",
    "        stderr.write('The duration of the period should be a non-negative number.')\n",
    "        raise SystemExit(1)\n",
    "    elif args.periods <= 0:\n",
    "        stderr.write('The number of periods should be a non-negative number.')\n",
    "        raise SystemExit(1)\n",
    "    elif args.summary <= 0:\n",
    "        stderr.write('The number of tweets should be a non-negative number.')\n",
    "        raise SystemExit(1)\n",
    "    elif args.sp <= 0:\n",
    "        stderr.write('The number of periods should be a non-negative number.')\n",
    "        raise SystemExit(1)\n",
    "    elif not (0 <= args.c <= 1):\n",
    "        stderr.write('The cosine similarity threshold should belong in range [0, 1].')\n",
    "        raise SystemExit(1)\n",
    "    elif args.threshold < 0:\n",
    "        stderr.write('The threshold should be a non-negative number.')\n",
    "        raise SystemExit(1)\n",
    "    return args\n",
    "\n",
    "\n",
    "def set_parameters(args):\n",
    "    Settings.TIME_WINDOW = args.duration * 60000\n",
    "    Settings.IGNORE_URLS = args.urls\n",
    "    Settings.IGNORE_RETWEETS = args.retweets\n",
    "    Settings.IGNORE_DUPLICATES = args.duplicates\n",
    "    Settings.IGNORE_USERNAME = args.mentions\n",
    "    Settings.STEMMING = args.stemming\n",
    "    Settings.TWEETS_SUMMARY = args.summary\n",
    "    Settings.SUMMARY_SIMILARITY = args.c\n",
    "    Settings.SUMMARY_PERIODS = args.sp\n",
    "    Settings.PREVIOUS_PERIODS = args.periods\n",
    "\n",
    "    if args.burst:\n",
    "        Settings.BURST_DETECTION_THRESHOLD[splitext(args.input)] = args.threshold\n",
    "    else:\n",
    "        Settings.EVENT_DETECTION_THRESHOLD[basename(args.input)] = args.threshold\n",
    "\n",
    "    with open(args.input, 'rb') as f:\n",
    "        for row in reader(f):\n",
    "            Settings.EVENT_TIMESTAMPS[basename(args.input)] = [int(row[0]), int(round(time() * 1000))]\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = argument_check()\n",
    "    set_parameters(args)\n",
    "    start(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
